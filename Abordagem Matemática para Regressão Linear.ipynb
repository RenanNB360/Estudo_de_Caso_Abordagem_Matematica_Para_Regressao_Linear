{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80530df6-6d84-481c-acf0-f6aaa2f439b8",
   "metadata": {},
   "source": [
    "# Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc736197-69e1-407d-9f0c-052235a3bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9165f12d-ee6f-41c9-9445-a52662e1543a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9984, 2976)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train = (10000 // batch_size) * batch_size\n",
    "val = (3000 // batch_size) * batch_size\n",
    "train, val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50680bbe-2529-44e7-ba58-0246f4396df3",
   "metadata": {},
   "source": [
    "**Explicação do código:**\n",
    "\n",
    "As linhas de código definem o tamanho do lote de dados como 32 e calculam o número total de amostras usadas para treinamento e validação, garantindo que sejam múltiplos exatos desse valor. O número de amostras de treinamento (*train*) é obtido dividindo 10000 por 32, pegando apenas a parte inteira do resultado e multiplicando novamente por 32, o que dá 9984 amostras. O mesmo processo é feito para as amostras de validação (*val*), dividindo 3000 por 32, resultando em 2976 amostras.\n",
    "\n",
    "Isso garante que todas as amostras sejam usadas em lotes completos, evitando problemas ao dividir os dados durante o treinamento e a validação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde4fdf-2c01-4e62-8961-e3157d603d16",
   "metadata": {},
   "source": [
    "# Implementação do Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b01d36-b41d-4aab-a25d-26638ada6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticRegressionData:\n",
    "\n",
    "    def __init__(self, w, b, noise=0.01, num_train=train, num_val=val, batch_size=batch_size):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.noise = noise\n",
    "        self.num_train = num_train\n",
    "        self.num_val = num_val\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        n = num_train + num_val\n",
    "        self.X = torch.randn(n, len(w))\n",
    "        noise = torch.randn(n, 1) * noise\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        indices = list(range(0, self.num_train)) if train else list(range(self.num_train, self.num_train + self.num_val))\n",
    "        random.shuffle(indices)\n",
    "        for i in range(0, len(indices), self.batch_size):\n",
    "            batch_indices = torch.tensor(indices[i: i + self.batch_size])\n",
    "            yield self.X[batch_indices], self.y[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a348212-bb9a-4812-ad3e-b7df93bdeec5",
   "metadata": {},
   "source": [
    "**Explicação do código:**\n",
    "\n",
    "Este código cria uma classe *SyntheticRegressionData* que gera dados sintéticos para um problema de regressão linear. Eu uso pesos *w* e um viés *b* para criar uma relação linear entre as variáveis de entrada *X* e a variável de saída *y*, com um ruído adicionado para simular dados reais. O construtor *(__init__)* inicializa os dados, criando *X* como uma matriz de valores aleatórios e *y* como uma combinação linear de *X*, *w* e *b*, com ruído adicionado.\n",
    "\n",
    "Eu também divido os dados em conjuntos de treino e validação, com tamanhos definidos por *num_train* e *num_val*. O método *get_dataloader* permite iterar sobre os dados em lotes (*batch_size*), embaralhando os *índices* para treino e retornando os pares *(X, y)* correspondentes. Esse código é útil para simular e testar modelos de regressão linear em um ambiente controlado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da903358-9896-4bb9-b21a-0a1b77d87966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch:\n",
    "\n",
    "    def __init__(self, num_inputs, lr, sigma=0.01):\n",
    "        self.lr = lr\n",
    "        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X, self.w) + self.b\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        return ((y_hat - y) ** 2 / 2).mean()\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.w, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c18c802-c6cc-4dfc-b214-407785a14208",
   "metadata": {},
   "source": [
    "**Explicação do código:**\n",
    "\n",
    "Este código implementa uma regressão linear do zero usando PyTorch. Eu inicializo os pesos w com\n",
    "valores aleatórios de uma distribuição normal e o viés b com zero, ambos com *requires_grad=True*\n",
    "para calcular gradientes. No método *forward*, calculo a previsão *y_hat* usando a fórmula $y_{hat} = X \\cdot$\n",
    "w + b, onde *torch.matmul(X, self.w)* faz a multiplicação de matrizes. \n",
    "\n",
    "A função *loss* calcula o erro quadrático médio dividido por 2: $Loss = \\frac{1}{2} \\cdot (y_{hat} - y)^2$, e retorno a média do erro. Por fim, o método *parameters* retorna os parâmetros w e b para atualização durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfc51b7-b4c7-4046-8b96-e602c92a359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.data = param.data - self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f9acb-6f46-4a2b-842f-7e8c80977acc",
   "metadata": {},
   "source": [
    "**Explicação do código:**\n",
    "\n",
    "Essa classe implementa o gradiente descendente estocástico (SGD). Na função *step*, eu atualizo os\n",
    "parâmetros do modelo usando a fórmula $param.data = param.data - lr \\cdot param.grad$, onde *lr* é a taxa de\n",
    "aprendizagem e *param.grad* é o gradiente do parâmetro. Isso move os parâmetros na direção que\n",
    "minimiza a função de perda.\n",
    "\n",
    "Já na função *zero_grad*, eu zero os gradientes dos parâmetros após cada\n",
    "atualização, para evitar que eles interfiram no próximo passo de treinamento. Essas duas funções são\n",
    "essenciais para o treinamento de modelos com gradiente descendente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52b920a7-f82b-4725-b6c0-9d4aef2ca87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, max_epochs, gradient_clip_val=0, decay_rate=0.000001):\n",
    "        self.max_epochs = max_epochs\n",
    "        self.gradient_clip_val = gradient_clip_val\n",
    "        self.decay_rate = decay_rate\n",
    "        self.step_count = 0\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        self.train_dataloader = list(data.get_dataloader(train=True))\n",
    "        self.val_dataloader = list(data.get_dataloader(train=False))\n",
    "\n",
    "    def prepare_model(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, model, data):\n",
    "        self.prepare_data(data)\n",
    "        self.prepare_model(model)\n",
    "        self.optim = SGD(model.parameters(), model.lr)\n",
    "        for epoch in tqdm(range(self.max_epochs), desc='Training'):\n",
    "            self.fit_epoch()\n",
    "\n",
    "    def fit_epoch(self):\n",
    "        train_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for X, y in self.train_dataloader:\n",
    "            y_hat = self.model.forward(X)\n",
    "            loss = self.model.loss(y_hat, y)\n",
    "            loss.backward()\n",
    "            self.step_count += 1\n",
    "            self.optim.lr *= (1 / (1 + self.decay_rate * self.step_count))\n",
    "            self.optim.step()\n",
    "            self.optim.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        train_loss /= num_batches\n",
    "        print(f'Époch: {self.step_count}, Train Loss: {train_loss:.4f}, Learning Rate: {self.optim.lr:.6f}')\n",
    "        \n",
    "        if self.val_dataloader:\n",
    "            val_loss = 0.0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for X, y in self.val_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    y_hat = self.model.forward(X)\n",
    "                    loss = self.model.loss(y_hat, y)\n",
    "                    val_loss += loss.item()\n",
    "                    num_batches += 1\n",
    "\n",
    "            val_loss /= num_batches\n",
    "            print(f'Époch: {self.step_count}, Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f9212-613d-47fe-b31b-161eeb8c67b8",
   "metadata": {},
   "source": [
    "**Explicação do código:**\n",
    "\n",
    "O *Trainer* é responsável por treinar um modelo de aprendizado de máquina\n",
    "utilizando gradiente descendente. No início, os valores de *max_epochs*,\n",
    "*gradient_clip_val* e *decay_rate* são definidos por padrão, e *step_count* é inicializado em zero.\n",
    "\n",
    "A função *prepare_data* carrega os dados de treinamento e validação usando\n",
    "*data.get_dataloader(train=True)* para o conjunto de treinamento e\n",
    "*data.get_dataloader(train=False)* para o conjunto de validação. A função\n",
    "*prepare_model* recebe o modelo que será treinado.\n",
    "\n",
    "A função *fit* inicia o treinamento chamando *prepare_data* e *prepare_model*. Em\n",
    "seguida, cria o otimizador *SGD* com os parâmetros do modelo e a taxa de aprendizado\n",
    "inicial. O treinamento ocorre em um loop de epochs, onde *fit_epoch* é chamado\n",
    "repetidamente.\n",
    "\n",
    "Dentro de *fit_epoch*, *train_loss* e *num_batches* são inicializados em zero. Para cada lote\n",
    "de dados de entrada $X$ e rótulos $y$ no conjunto de treinamento, a previsão do modelo\n",
    "é calculada como $\\hat{y} = Xw + b$. A perda é então obtida com a fórmula $loss = \\frac{1}{2}(\\hat{y} -\n",
    "y)^2$.\n",
    "\n",
    "O método *loss.backward()* computa os gradientes e incrementa *step_count*. A taxa de\n",
    "aprendizado é atualizada de acordo com $lr = \\frac{lr}{1+decay\\_rate \\cdot step\\_count}$, reduzindo\n",
    "gradativamente. Os parâmetros do modelo são ajustados usando $param = param - lr \\cdot param.grad$, e os gradientes são zerados para evitar acumulação.\n",
    "\n",
    "A perda média do treinamento é calculada e exibida na tela junto com a taxa de\n",
    "aprendizado. Se houver um conjunto de validação, a perda de validação é calculada sem\n",
    "atualizar os parâmetros, usando $loss = \\frac{1}{2}(\\hat{y} - y)^2$, e o resultado também é exibido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f521196-cf53-444b-8bad-116c4c0ed003",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa7992-0eca-4134-a02c-09fa508adb36",
   "metadata": {},
   "source": [
    "**Explicação do código:**\n",
    "\n",
    "O *true_w* recebe os valores dos pesos e o *true_b* recebe o valor do viés (bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c96e36b-f601-439d-b7ad-5f0b5622fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SyntheticRegressionData(w=true_w, b=true_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80baca79-91b4-4fc8-90bf-e200c7f67811",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionScratch(num_inputs=2, lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0cf6f8a-fbc8-438d-86fb-dad447c67868",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(max_epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2af42c-7990-4f67-b9fa-db4e122d4c0f",
   "metadata": {},
   "source": [
    "**Explicação do código:**\n",
    "\n",
    "A primeira linha cria um conjunto de dados sintético para regressão, onde os pesos verdadeiros dos dados são definidos por *true_w* e o viés verdadeiro por *true_b*.\n",
    "\n",
    "A segunda linha instancia um modelo de regressão linear do zero, especificando que ele terá duas entradas e uma taxa de aprendizado inicial de 0.01. \n",
    "\n",
    "A terceira linha cria um treinador que irá treinar o modelo por 4 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa218c90-5137-4ede-b7ee-d3c6b77edd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 17.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Époch: 312, Train Loss: 2.6760, Learning Rate: 0.009523\n",
      "Époch: 312, Validation Loss: 0.0307\n",
      "Époch: 624, Train Loss: 0.0052, Learning Rate: 0.008229\n",
      "Époch: 624, Validation Loss: 0.0001\n",
      "Époch: 936, Train Loss: 0.0001, Learning Rate: 0.006451\n",
      "Époch: 936, Validation Loss: 0.0001\n",
      "Époch: 1248, Train Loss: 0.0001, Learning Rate: 0.004588\n",
      "Époch: 1248, Validation Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3268a-a18f-4709-b771-9eaf9fbc06ad",
   "metadata": {},
   "source": [
    "**Explicação do treinamento:**\n",
    "\n",
    "A primeira linha chama o método de treinamento, iniciando o processo de ajuste do modelo aos dados. O treinamento ocorre por quatro épocas, conforme especificado anteriormente. Durante esse processo, a barra de progresso mostra que todas as épocas foram concluídas rapidamente. \n",
    "\n",
    "A cada conjunto de 312 iterações, são exibidos os valores de perda para os dados de treino e validação, além da taxa de aprendizado ajustada. No início, a perda no treino é alta, mas reduz rapidamente à medida que o modelo aprende, enquanto a perda na validação também diminui e se mantém muito baixa. A taxa de aprendizado diminui gradualmente ao longo das iterações, o que ajuda o modelo a ajustar os pesos com mais precisão à medida que se aproxima de um bom ajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c040c9c-0a62-481e-9ce2-879b4b2a9131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro na estimação de w: tensor([-6.2466e-05, -2.2578e-04])\n",
      "Erro na estimação de b: tensor([0.0002])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(f'Erro na estimação de w: {true_w - model.w.reshape(true_w.shape)}')\n",
    "    print(f'Erro na estimação de b: {true_b - model.b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d944ded3-cbbd-480a-b501-1ce721465868",
   "metadata": {},
   "source": [
    "**Conclusão do estudo de caso:**\n",
    "\n",
    "O código desenvolvido implementa uma rede neural para regressão linear do zero, adotando uma abordagem matemática estruturada. Os dados sintéticos foram gerados com pesos e viés conhecidos, permitindo avaliar a precisão do modelo. O treinamento ocorreu de forma eficiente, com uma rápida convergência da função de perda, enquanto a validação demonstrou que o modelo generaliza bem.\n",
    "\n",
    "A otimização com SGD e decaimento da taxa de aprendizado contribuiu para um ajuste preciso dos parâmetros. O resultado final confirma o sucesso da abordagem, com erros mínimos na estimativa dos pesos e do viés, demonstrando que o modelo aprendeu corretamente os valores esperados. Esse estudo de caso reforça a importância do entendimento matemático na construção de redes neurais do zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
